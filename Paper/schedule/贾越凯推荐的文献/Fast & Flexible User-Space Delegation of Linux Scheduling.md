

Abstract

ghOSt是一种基础设施，用于将内核调度决策委托给用户空间代码。它旨在支持数据中心工作负载和平台的快速发展需求。

优化调度决策可以显著提高重要工作负载的吞吐量、尾延迟、可扩展性和安全性。然而，内核调度程序难以高效地实现、测试和部署到大型机群中。最近的研究表明，在自定义数据平面操作系统中使用定制调度策略可以在数据中心环境中提供令人信服的性能结果。然而，由于在应用程序粒度上部署自定义操作系统映像在多租户环境中不切实际，因此这些新技术的实际应用受到了限制。

ghOSt是一个通用的Linux环境下的调度策略委派工具，可以将调度策略委派给用户空间进程。它支持多种调度模型，包括分布式、抢占式等，并且可以在不重启主机的情况下修改策略。使用ghOSt可以实现与内核调度器相当的吞吐量和延迟，同时还能进行策略优化、非破坏性升级和故障隔离。该工具已经开源，以便未来的研究和开发。

本文介绍了软件工程的概念，包括软件及其工程。

关键词：操作系统、线程调度。文本提到了操作系统和线程调度的相关内容。

ACM Reference Format:

这篇文章介绍了ghOSt，一种快速灵活的用户空间委派Linux调度的方法。该方法在2021年的ACM SIGOPS第28届操作系统原理研讨会上发表。

1 Introduction

CPU调度对应用程序的性能和安全性起着重要作用。针对特定的工作负载类型定制策略可以显著提高关键指标，如延迟、吞吐量、硬/软实时特性、能源效率、缓存干扰和安全性。为了缓解最近的硬件漏洞，云平台必须快速部署新的核心隔离策略，隔离应用程序之间的共享处理器状态。

设计、实施和部署新的调度策略是一项艰巨的任务，需要开发人员设计能够平衡多个应用程序的性能要求的策略。实现必须符合复杂的内核架构，错误会导致整个系统崩溃或者由于意外的副作用严重影响性能。即使成功，升级的破坏性本身也会带来主机和应用程序停机的机会成本。这在风险最小化和进步之间产生了挑战性的冲突。

以前的尝试通过设计用户空间解决方案来改善性能和减少内核复杂性存在重大缺点：它们需要大量修改应用程序实现，需要专用资源以保持高度响应性，或者需要特定于应用程序的解决方案。

PAGE 2

该文提到了对内核进行修改的相关研究，包括编号为1、2、10、21、25、34-37的研究。

Linux支持多种调度实现，但在大型系统中为每个应用程序定制、维护和部署不同的机制是不切实际的。我们的目标之一是规范哪些内核更改需要在稳定的内核ABI上启用大量优化（和实验）。

硬件环境不断变化，现有的调度抽象模型已经无法管理复杂的系统。调度器必须不断发展，以支持增加的核心数量、新的共享属性和异构系统。我们需要一种新的范式来更有效地支持不断变化的问题领域，而不是使用单片内核实现。

本文介绍了ghOSt的设计和在生产和学术用例中的评估。ghOSt是一个本地操作系统线程调度程序，将策略决策委托给用户空间。ghOSt的目标是从根本上改变调度策略的设计、实现和部署方式。ghOSt提供了用户空间开发的灵活性和部署的便利性，同时仍然支持微秒级别的调度。ghOSt为用户空间软件提供了抽象和接口，以定义复杂的调度策略，从每个CPU模型到系统范围（集中式）模型。重要的是，ghOSt将内核调度机制与策略定义分离。机制驻留在内核中，很少更改。策略定义驻留在用户空间中，快速更改。我们开源我们的实现，以便未来使用ghOSt进行研究和开发。

ghOSt是一个支持现有应用程序的本地线程调度器，无需进行任何更改。它使用**代理进程**来运行调度策略逻辑，并通过ghOSt API与内核交互。**内核通过消息队列异步通知代理进程所有受管理的线程的状态变化，代理进程使用事务式API同步提交线程的调度决策**。ghOSt支持多个策略的并发执行、故障隔离和为不同应用程序分配CPU资源。为了实现现有系统向ghOSt的实际过渡，它与其他内核调度器（如CFS）共存。

ghOSt是一个调度器，可以定义各种策略，其性能与现有的调度器相当或更好。ghOSt的关键操作开销很小，包括**消息传递、上下文切换和线程调度**。通过分摊，一个ghOSt代理每秒可以调度超过200万个线程。我们通过实现集中式和抢占式策略来评估ghOSt，这些策略在高请求分散和敌对情况下实现了高吞吐量和低尾延迟。与专门的现代数据平面Shinjuku相比，ghOSt的开销很小，性能相当（在5%以内），同时支持更广泛的工作负载，包括多租户。

该研究实现了一个名为ghOSt的通用调度策略，可以在不需要大量内核修改的情况下实现调度策略。该策略在Snap、Google Search和虚拟机等场景中的性能表现都很好，可以提高尾延迟和吞吐量。同时，该策略还可以保护系统免受最近发现的微架构漏洞的攻击。

2 Background & Design Goals

大型云服务提供商和用户需要部署新的调度策略来优化关键工作负载的性能，同时通过将不受信任的线程隔离在单独的物理核心上，提供对新硬件漏洞（如Spectre和L1TF/MDS）的保护。

Linux支持通过调度类实现多种策略，开发者可以修改调度类以优化特定应用程序的性能。现有的调度类设计为支持尽可能多的用例，但这也使得使用这些过于通用的类来优化高性能应用程序变得具有挑战性。因此，许多开发者选择使用现有的通用策略，如Linux完全公平调度器（CFS）。同时，云提供商或应用程序开发者也可以创建全新的类/策略，以针对特定服务进行优化。

PAGE 3

实现调度器很困难，开发人员在设计新的内核调度器时发现它们难以实现、测试和调试。调度器通常使用低级语言（如C和汇编）编写，无法利用有用的库（如Facebook Folly和Google Abseil），也无法使用流行的调试工具进行内省。此外，调度器依赖于复杂的同步原语，包括原子操作、RCU、任务抢占和中断，使得开发和调试变得更加困难。长期维护也是一个挑战，Linux很少合并新的调度类，并且不太可能接受高度调优的非通用调度器。因此，自定义调度器在Linux外部进行维护，随着上游Linux的演进而不断合并。

部署调度程序非常困难，需要在大型机群中部署新内核。云服务提供商很难忍受内核更新频率低于一个月。安装新内核需要迁移或终止机器的工作，安装新系统软件，等待系统守护进程重新初始化，最后等待新分配的应用程序再次准备好服务。**ghOSt可以在不更新内核和/或重新启动机器和应用程序的情况下更新、测试和调整调度程序。**

ghOSt是一个在用户空间运行的内核调度程序，用于调度本地操作系统线程。与此相反，用户级线程运行时调度用户线程。这些运行时在N个本地线程上复用M个用户线程，这是不可预测的：尽管用户空间运行时可以控制哪个用户线程在给定的本地线程上运行，但它无法控制本地线程何时被调度实际运行或在哪个CPU上运行。更糟糕的是，内核可以取消调度持有用户级锁的线程。为了克服这个限制，开发人员有两个选择。一种是将CPU专用于运行用户线程的本地线程，从而保证隐式控制。然而，这种选择在低工作负载利用率时会浪费资源，因为专用的CPU不能与另一个应用程序共享（参见§4.2），并且需要大量协调来扩展容量。或者，开发人员可以任由本地线程调度程序，允许CPU共享，但最终失去了他们转向用户级运行时的响应时间控制。ghOSt通过保证对响应时间的控制，同时允许灵活共享CPU资源，实现了两全其美。

定制调度器/数据平面操作系统对于每个工作负载来说是不切实际的。之前的研究实现了高度专业化的数据平面操作系统，为特定的网络工作负载提供了定制的调度策略和网络堆栈。虽然这些系统为其目标提供了良好的性能，但它们的内核实现不能轻易更改，并且对于其他工作负载提供了较差的性能。这些系统的代码量较大，无法与其他应用程序共存，并且无法在没有特定网卡的机器上运行。

通过BPF进行自定义调度是不够的。将BPF程序注入内核调度器是一种吸引人的自定义内核调度的方式，但BPF在表达能力和访问内核数据结构方面存在限制。例如，BPF验证器必须能够确定循环将退出，BPF程序不能使用浮点数。因此，需要更强大的工具来实现自定义内核调度。

BPF程序必须同步运行，需要快速响应调度事件，而异步调度器可以在稍后的时间内响应这些事件。异步模型可以基于系统的更广泛视角做出调度决策。然而，BPF在ghOSt中扮演了重要角色，加速快速路径操作并在内核中的性能关键点提供定制化。

2.1 Design Goals

PAGE 4

ghOSt旨在引入一种新的设计、优化和部署调度器的范式。设计时考虑以下要求：

政策应该易于实施和测试。随着新的工作负载、内核和异构硬件进入数据中心，实施新的调度策略不应该需要另一个内核补丁。如DashFS所示，将系统实现在用户空间而不是内核空间简化了开发，并且可以使用流行的语言和工具进行更快的迭代。

云服务提供商和用户需要表达调度策略，以满足各种优化目标，包括吞吐量导向的工作负载、微秒级延迟关键的工作负载、软实时和能源效率要求。

Linux调度器生态系统实现了基于每个CPU的调度算法，但最近的研究表明，通过集中式的专用轮询调度线程可以实现微秒级工作负载的尾延迟改进。其他系统也证明了在比单个CPU更粗粒度的层面上协调的好处。因此，ghOSt应该支持将调度决策委托给远程CPU，以支持从基于每个CPU到集中式和介于两者之间的所有调度模型。

ghOSt需要支持多个并发策略，随着数百个核心和新的加速器的横向扩展，机器容量不断增加，支持在单个服务器上运行多个负载和租户。ghOSt必须支持分区和共享机器，以便多个调度策略可以并行执行。

操作系统升级会导致昂贵的停机时间和计算能力降低，因此需要非破坏性更新和故障隔离。调度策略应与主机内核分离，允许部署、更新、回滚或崩溃的新策略，而不会导致机器重启成本。

3 Design

我们现在讨论ghOSt的设计和实现，解释了它们如何满足我们在第2节中列出的要求。

ghOSt是一个基于用户空间代理的调度系统，其内核部分实现为一个调度类，提供了丰富的API来定义任意调度策略。代理通过消息和状态字来获取线程状态，并通过事务和系统调用来指示内核进行调度决策。

本节将使用两个例子来说明：基于CPU的调度和集中式调度。传统的内核调度策略，如CFS，是基于CPU的调度器。虽然这些策略通常采用负载平衡和工作窃取来平衡系统负载，但它们仍然从基于CPU的角度操作。集中式调度的例子类似于Shinjuku [25]、Shenango [1]和Caladan [21]提出的模型。在这种情况下，有一个单一的全局实体不断观察整个系统，并为其管辖范围内的所有线程和CPU做出调度决策。

ghOSt在CPU上调度本地线程。本节中提到的所有线程都是本地线程，与第2节中提到的用户级线程相对。我们将逻辑执行单元称为CPU。例如，我们认为具有56个物理核心和112个逻辑核心（超线程）的机器具有112个CPU。

ghOSt支持在单台机器上使用enclaves运行多个并发策略。可以将系统分割成多个独立的enclaves，每个enclave以CPU为粒度运行自己的策略。从调度的角度来看，enclaves是隔离的。分区在单台机器上运行不同的工作负载时是有意义的。通常可以通过机器拓扑（如每个NUMA套接字或每个AMD-CCX）来设置这些enclaves的粒度。enclaves还有助于隔离故障，将代理崩溃的损害限制在其所属的enclave中。

PAGE 5

ghOSt用户空间代理实现了调度策略逻辑，代理可以用任何语言编写和调试，实现更容易。如果代理崩溃，系统将回退到默认调度程序，如CFS，保证了容错性和隔离性。在启动新的ghOSt用户空间代理时，机器仍然完全可用。

由于崩溃恢复性质，更新调度策略只需重新启动用户空间代理，而无需重新启动机器。这个性质使得对各种硬件和工作负载进行实验和快速策略定制成为可能。开发者可以进行策略微调并简单地重新启动代理。在第3.4节中讨论了ghOSt策略的动态更新。

ghOSt的每个CPU都有一个本地代理，无论是按CPU还是集中式调度模型。在按CPU调度模型中，每个代理负责自己CPU的线程调度决策。在集中式调度模型中，一个全局代理负责调度enclave中的所有CPU，其他本地代理处于非活动状态。每个代理都是在Linux pthread中实现的，并且所有代理都属于同一个用户空间进程。

3.1 Kernel-to-Agent Communication

为了让代理程序对其监管的线程做出调度决策，内核必须向代理程序公开线程状态。一种方法是将现有的内核数据结构（如task_structs）映射到用户空间，以便代理程序可以检查它们以推断线程状态。另一种方法是通过sysfs文件公开线程状态，但文件系统API效率低下，难以支持微秒级别的策略。

我们需要一个快速且不依赖于线程的底层内核实现的内核-用户空间API。受分布式系统的启发，我们使用消息作为高效且简单的解决方案。

ghOSt是一个内核，它使用表1中列出的消息通知用户空间代理线程状态的变化。例如，如果一个线程被阻塞并且现在准备运行，内核会发布一个THREAD_WAKEUP消息。此外，内核还使用TIMER_TICK消息通知代理定时器滴答。为了帮助代理验证它们基于最新状态做出决策，消息还具有序列号，稍后将进行解释。

消息队列是将消息传递给代理的方式。每个线程都有一个队列，所有关于该线程状态变化的消息都会被传递到该队列中。在分布式系统中，每个线程被分配到对应的CPU队列中。在集中式系统中，所有线程都被分配到全局队列中。CPU事件的消息会被路由到与CPU相关的代理线程的队列中。

我们选择使用共享内存中的自定义队列来高效处理代理程序的唤醒。我们认为现有的队列机制对于ghOSt来说是不足的，因为它们只存在于特定的内核版本中。例如，BPF系统通过BPF环形缓冲区将BPF事件传递给用户空间[65]，而最新版本的Linux也通过io_uring将异步I/O消息传递给用户空间[66]。这些都是快速无锁的环形缓冲区，用于同步消费者/生产者访问。然而，旧版本的Linux内核和其他操作系统不支持它们。

ghOSt初始化后有一个默认队列，代理进程可以使用CREATE/DESTROY_QUEUE() API创建/销毁队列。线程被隐式分配到默认队列中，代理可以通过ASSOCIATE_QUEUE()更改分配。

队列可以配置为在消息产生时唤醒一个或多个代理。代理可以通过CONFIG_QUEUE_WAKEUP()配置唤醒行为。在每个CPU的例子中，每个队列与一个CPU关联，并配置为唤醒相应的代理。在集中式的例子中，队列由全局代理持续轮询，因此唤醒是多余的，不需要配置。消息进入队列并在代理中观察到的延迟在第4.1节中讨论。

Agent wakeup使用标准内核机制唤醒被阻塞的线程。这包括识别要唤醒的代理线程，将其标记为可运行状态，可选择向目标CPU发送中断以触发重新调度，并执行上下文切换到代理线程。

为了实现负载均衡和工作窃取，代理可以通过ASSOCIATE_QUEUE()改变消息从线程到队列的路由。如果在原始队列中有待处理的消息，线程的关联更改将失败。在这种情况下，代理必须在重新发出ASSOCIATE_QUEUE()之前排空原始队列。

PAGE 6

代理与内核的同步。代理通过消息对系统状态进行操作。然而，在代理进行调度决策时，新的消息可能会到达队列，从而改变该决策。为了解决这个挑战，我们使用代理/线程序列号来解决：每个代理都有一个序列号，当消息被发布到与该代理相关联的队列时，该序列号会递增。我们在第3.2节中解释了在perCPU示例中如何使用代理序列号。每个线程T都有一个序列号Tseq，当该线程发布一个新的状态改变消息MT时，该序列号会递增。当代理弹出队列时，它会收到消息和相应的序列号：(MT, Tseq)。我们在第3.3节中解释了在集中式调度示例中如何使用Tseq。

ghOSt通过共享内存暴露序列号，使代理程序能够高效地轮询线程和CPU状态的辅助信息。内核更新线程或代理程序的序列号时，也会更新相应的状态字。代理程序可以从共享映射中的状态字中读取序列号。

3.2 Agent-to-Kernel Communication

描述了代理如何指示内核选择下一个要调度的线程。

代理通过提交事务向内核发送调度决策。代理需要能够调度本地CPU和其他远程CPU。对于每个CPU的情况，系统调用接口足以支持。对于集中式情况，代理需要有效地向多个CPU发送调度请求，并检查这些请求是否成功。共享内存接口更适合。使用共享内存中的事务作为调度接口将允许将调度决策卸载到具有对该内存的访问权限的外部设备中。

该文介绍了一种基于共享内存的事务API，灵感来源于事务内存和数据库系统。该API支持快速、分布式的原子提交操作，可以同时针对同一远程节点进行多个提交。ghOSt代理需要类似的属性。代理使用TXN_CREATE()函数在共享内存中打开一个新的事务，并将要调度的线程的TID和要调度线程的CPU的ID写入其中。当事务填充完毕后，代理通过TXNS_COMMIT()系统调用将其提交给内核，触发内核启动提交过程并触发上下文切换。

为了让ghOSt能够扩展到数百个CPU和每秒数十万个事务，我们必须减少系统调用的昂贵成本。通过引入组提交，我们可以分摊事务的成本。组提交还可以减少发送给其他CPU的中断数量。代理通过将所有事务传递给TXNS_COMMIT()系统调用来提交多个事务。这个系统调用将昂贵的开销分摊到多个事务上。最重要的是，它通过使用大多数处理器中的批量中断功能来分摊发送中断的开销。内核不再发送多个中断（每个事务一个），而是发送一个批量中断到远程CPU，从而节省了大量开销。

本文介绍了在per-CPU例子中如何通过序列号来解决事务和交易的问题。当代理提交事务时，它将CPU让给目标线程进行调度。在代理运行时，发布到队列中的消息不会引起唤醒，因为代理已经在运行。但是，队列中的新消息可能来自更高优先级的线程，如果代理意识到它，将影响调度决策。代理只有在下一次唤醒时才有机会检查该消息，这太晚了。本文解释了如何通过序列号来解决这个问题。对于集中式调度，本文在第3.3节中介绍了略有不同的情况。

PAGE 7

使用代理序列号来解决挑战。代理通过检查代理线程的状态字来获取其代理序列号。当向与代理相关联的队列发布新消息时，代理序列号会递增。操作顺序为：1）读取代理序列号；2）从队列中读取消息；3）进行调度决策；4）将代理序列号与事务一起发送到TXNS_COMMIT()。如果事务中发送的代理序列号早于内核观察到的当前代理序列号（即，代理的队列中有新消息发布），则事务被视为“过时”，并将以ESTALE错误失败。然后，代理会清空其队列以检索更新的消息，并重复该过程。

使用BPF加速调度。ghOSt提供的用户级灵活性并不是免费的：消息传递和组调度需要花费高达5微秒的时间；在集中式调度模型中，线程可能需要等待整个集中式调度循环，直到为其做出调度决策，这需要30微秒的时间。

ghOSt是一个可以通过自定义BPF程序来恢复丢失的CPU时间的代理程序。当CPU变为空闲状态且代理程序尚未发出事务时，BPF程序会发出自己的事务，选择一个线程在该CPU上运行。BPF程序通过共享内存窗口与代理程序通信。ghOSt BPF程序实质上是代理程序本身的扩展，因此BPF字节码嵌入在代理程序二进制文件中，使用libbpf。具体的调度策略由代理程序使用BPF基础设施来安排线程在CPU上运行。

3.3 The Centralized Scheduler

构建集中调度策略需要解释额外的实施细节。

ghOSt采用单一全局代理和单一队列进行集中调度，该代理轮询单一消息队列并为所有受其管理的CPU做出调度决策。尽管集中策略可能看起来无法支持微秒级别的调度，但在生产工作负载上，ghOSt表现出相当或更好的整体性能。

为了支持微秒级调度，全局代理必须持续运行，避免被更高优先级的内核调度类抢占。为了防止这种情况，ghOSt将所有代理分配高内核优先级，但这可能会不稳定系统。因此，必须小心处理，以确保系统稳定。

ghOSt通过让所有不活跃的代理立即让出CPU来维护系统的稳定性。当非ghOSt线程需要在全局代理的CPU上运行时，全局代理会将其“热交接”给另一个空闲的CPU上的不活跃代理。这样可以保证系统的稳定性和线程的顺畅运行。

全局代理可能会出现线程状态不一致的情况，需要一种机制来确保在调度线程时，如果有线程亲和性的限制，事务将失败。

全局代理需要支持数千个线程，不像本地代理那样放弃自己的CPU。全局代理只需要验证它是否与当前调度的线程T保持最新即可。可以使用代理序列号来实现。

PAGE 8

使用线程序列号解决问题。每个排队的消息都会标记上线程序列号。当代理为线程T提交事务时，它会将事务和它所知道的线程T的最新序列号一起发送。内核接收到事务后，会验证线程序列号是否与事务中的线程一致。如果不一致，则事务失败并返回ESTALE错误。

3.4 Fault Isolation and Dynamic Upgrades

ghOSt的设计目标之一是易于在现有系统上采用。即使ghOSt策略有缺陷，我们仍希望ghOSt管理的线程与系统中的其他线程良好交互。我们希望避免ghOSt线程对其他线程造成意外后果，如饥饿、优先级反转、死锁等。

通过将ghOSt的内核调度类别的优先级设置为低于默认调度类别，大多数线程将抢占ghOSt线程。ghOSt线程被抢占会触发THREAD_PREEMPT消息，引发相关代理程序做出调度决策。代理程序进一步决定如何处理抢占。

ghOSt实现了动态升级和回滚，更新调度策略不需要重启内核或应用程序，可以快速部署。在计划的代理更新或意外代理崩溃期间，长时间运行的应用程序仍然可以正确运行。ghOSt通过替换代理或重新启动来实现动态升级。

ghOSt支持在不破坏enclave的情况下更新代理。用户空间代码可以查询和epoll代理是否连接到enclave。为了升级代理，同时运行旧代理和新代理；新代理会阻塞，直到旧代理崩溃或退出并不再连接。如果此过程失败，内核或用户空间代码可以销毁enclave。销毁enclave会杀死该enclave中的所有代理，保持系统中的其他enclave完好无损，并自动将销毁的enclave中的所有线程移回CFS。此时，线程仍然正常运行，但由CFS而不是ghOSt进行调度。

ghOSt watchdog是一个安全机制，用于检测和处理ghOSt或其他内核调度器中的错误。调度错误可能导致系统范围的后果，例如，一个ghOSt线程在持有内核互斥锁时可能被抢占，如果它长时间不被调度，可能会导致其他线程（包括CFS或其他ghOSt enclave中的线程）停滞。同样地，如果关键线程（如垃圾收集器和I/O轮询器）没有被调度，整个系统将停止运行。为了安全起见，ghOSt会自动销毁具有异常行为的enclave。例如，当内核检测到一个agent在用户可配置的毫秒数内没有调度可运行的线程时，将销毁该enclave。

4 Evaluation

我们对ghOSt的评估主要关注三个问题：(a) ghOSt特定操作的开销，这些操作在传统调度器中不存在；(b) 使用ghOSt实现的调度策略与之前的工作（如Shinjuku）相比如何表现；(c) ghOSt是否适用于大规模和低延迟的生产工作负载，包括Google Snap、Google Search和虚拟机。

4.1 Analysis of ghOSt Overheads and Scaling

ghOSt是一个生产就绪的调度程序，比Linux CFS调度程序少了40%的内核代码。它支持多种调度策略，可以使用用户空间库中的常用函数来实现。使用高级语言进行策略定义的优势在于更灵活的抽象，使得复杂性可以集中在调度决策上。

实验环境为Linux 4.15，使用ghOSt补丁。微基准测试在2个Intel Xeon Platinum 8173M处理器上运行，每个处理器有28个核心和2个逻辑核心。

PAGE 9

表3总结了ghOSt独有的基本操作的开销，并报告了在CFS下等效线程操作的开销。

在每个CPU的例子中，将消息添加到队列中，切换到本地代理并出队消息构成了传递给本地代理的开销，其中上下文切换占据了主要部分。而在集中式的例子中，将消息添加到队列中并在全局代理中出队消息（全局代理一直在运行）构成了传递给全局代理的开销。

本文介绍了基于每个CPU的事务内存模型中的本地调度，包括提交事务和执行上下文切换的开销，以及与CFS上下文切换开销的比较。本地调度的开销为888纳秒，略高于CFS上下文切换开销，但仍具有竞争力。

在集中调度模型中，代理端提交事务并发送中断请求，目标CPU处理中断请求并执行上下文切换。代理的开销为668纳秒，每个代理的理论最大吞吐量为10-9 / 668 = 1.5M个调度线程每秒。将10个不同CPU的事务分组可以将理论最大吞吐量提高到10 * 10-9 / 3964 = 2.52M个调度线程每秒，通过分摊中断请求的开销。

一个代理人在100个CPU服务器上可以每秒安排大约25200个线程。如果线程长度为40微秒，代理人可以保持100个CPU忙碌。策略开发人员在设计ghOSt策略时应该考虑到每个代理人的可扩展性限制。这个限制随着更多代理人的增加而相对线性改善。

为了展示全局代理的可扩展性，我们分析了一个简单的轮询策略。该策略通过FIFO运行队列管理所有线程，并在CPU空闲时将它们调度到CPU上。代理每次提交尽可能多的事务。我们在默认的微基准机器上运行了实验，该机器使用Skylake处理器，以及一个Haswell处理器的2插槽机器（每插槽18个物理核心，每个核心两个逻辑核心，2.3 GHz）。

图5展示了结果。全局代理随着可用CPU数量的增加而增加每秒事务数，但当与ghOSt线程共享物理核心时，性能会下降。最后，调度远程套接字上的CPU会导致性能下降。

4.2 Comparison to Custom Centralized Schedulers

本文比较了ghOSt和一种高度专业化的调度系统，后者使用集中策略来调度要求高的微秒级工作负载。实验在一台2插槽Intel Xeon CPU E5-2658的单插槽上运行，每个插槽有12个核心，每个核心有24个逻辑核心，时钟频率为2.2 GHz。

本文比较了三种Shinjuku调度方法的实现，均用于RocksDB工作负载。使用一个物理核心进行负载生成，其中Shinjuku系统运行在Linux 4.4上，其他系统（ghOSt-Shinjuku和CFS-Shinjuku）在Linux 4.15上运行，并应用了我们的ghOSt补丁。

该系统使用20个旋转工作线程和一个旋转调度线程，运行在一个专用的物理核心上。旋转线程防止其他线程在它们的CPU上运行。调度程序按照先进先出的原则管理到达的请求，并将它们分配给工作线程。每个请求在有限的运行时间内运行，然后被抢占并添加到FIFO队列的末尾。

该研究在ghOSt中使用了Shinjuku调度策略，使用了710行用户空间代码。该策略使用集中式模型，维护了一个200个工作线程的池子，并使用FIFO队列将可运行的工作线程调度到20个CPU上。同时，ghOSt允许系统中的其他负载使用任何空闲的CPU。

PAGE 10

实现了一个非抢占版本的Shinjuku，运行在Linux CFS上，但不具备Shinjuku的专门数据平面特性，如虚拟化特性和预先中断。

本文介绍了一个单一工作负载比较实验，其中每个请求包括对内存中的RocksDB键值存储的GET查询和少量处理。处理时间分配为99.5％的请求为4微秒，0.5％的请求为10毫秒。每个工作线程的时间片为30微秒。CFS-Shinjuku是非抢占式的，因此所有请求都运行到完成。

图6a展示了我们的结果。ghOSt在𝜇 s级别的尾部工作负载方面与Shinjuku相当，尽管其Shinjuku策略的代码行数比自定义Shinjuku数据平面系统少82％。ghOSt在高负载下的尾部延迟略高于Shinjuku，并且饱和吞吐量与Shinjuku相差不到5％。CFS-Shinjuku由于缺乏抢占而比其他两个系统早饱和约30％。

在生产场景中，当RocksDB负载较低时，可以利用空闲的计算资源来处理低优先级的批处理应用程序或运行无服务器函数。然而，当将批处理应用程序与由Shinjuku管理的RocksDB工作负载共存时，即使RocksDB负载较低，批处理应用程序也无法获得任何CPU资源。

为了实现低延迟和批处理工作负载的安全共存，可以考虑使用以线程为导向的集中调度系统，例如Shenango。

Shenango的集中式调度程序监视网络应用的负载，当应用程序负载较轻时，调度程序将空闲的CPU周期分配给批处理应用程序。然而，Shenango不适用于执行时间变化的请求，因此RocksDB工作负载的尾延迟比Shinjuku要差得多。

该研究将ghOSt-Shinjuku策略扩展到仅17行代码，实现了Shenango风格的调度。该策略监控RocksDB的负载，并将空闲周期分配给批处理应用程序。修改后的ghOSt策略与原始ghOSt策略产生相同的尾延迟，同时与CFS相似，批处理应用程序可以利用相同数量的计算资源。这些改进不需要任何应用程序更改。

4.3 Google Snap

本文评估ghOSt作为我们软实时内核调度程序MicroQuanta的替代品，后者管理Snap的工作线程，Snap是我们的用户空间数据包处理框架。

Snap类似于DPDK，它维护了负责与NIC硬件交互并代表重要服务运行自定义网络和安全协议的轮询（工作）线程。Snap可以根据网络负载的变化决定是否生成/加入工作线程。

Snap使用至少一个工作线程不断轮询，随着网络负载的突发到来，Snap可能会唤醒并随后使其他工作线程进入睡眠状态。为了避免增加延迟，这些频繁的唤醒/睡眠需要快速的调度器干预。Snap部署了一个自定义的软实时调度器MicroQuanta，它保证每个数据包处理工作线程在任何时间段内最多只能获得一定的时间量，以确保工作线程获得运行时间而不会饿死其他线程。然而，这也会导致网络黑屏长达0.1毫秒。

PAGE 11

实验设置：我们使用了两台机器，每台机器配备了两个Intel Xeon Platinum 8173M处理器（每个插槽28个物理核心，每个核心2个逻辑核心，主频2GHz），383GB的DRAM和一个100Gbps的网络接口卡和匹配的交换机。我们的测试使用了单个插槽，即每台机器有56个逻辑CPU。

测试工作负载包括六个客户端线程和六个服务器线程。客户端线程每秒发送10k条消息到另一台机器上的服务器线程，并接收相同大小的回复。测试旨在测试线程调度而不是网络接口。其中一个客户端线程发送64字节的消息，其他五个客户端线程发送64kB的消息。在所有实验中，客户端和服务器线程由Linux CFS调度。在ghOSt实验中，工作线程由ghOSt而不是MicroQuanta调度。总带宽为51.86Gbps。

我们在两种模式下进行测试。在安静模式下，客户端/服务器线程是机器上唯一明确的工作负载。在负载模式下，机器还运行着40个额外的对抗线程，这些线程尝试在网络流量较低时使用空闲的CPU资源，这些资源未被服务器或Snap线程使用。

ghOSt策略是一个简单而有效的集中式FIFO策略，用于管理Snap的工作线程和对手线程。全局代理尝试找到一个空闲的CPU来调度其线程，优先考虑Snap的工作线程。在安静模式测试中，工作线程经常被客户端/服务器线程和其他由CFS调度的本地线程抢占。在负载模式测试中，Snap的工作线程会抢占对手线程，但不能抢占由CFS管理的线程。对手线程只在CFS线程和Snap都有多余资源时运行。没有使用专用核心。

本文比较了两种调度器在处理64B和64kB消息时的尾延迟。对于64B消息，ghOSt的性能与基准相似或略好，但在99.99%及以上的尾延迟方面，ghOSt的性能最多差1.7倍。对于64kB消息，ghOSt在99.9%及以上的尾延迟方面表现更好，尾延迟降低了5%至30%。这是因为64kB消息需要更多的处理，而ghOSt可以在CPU变得繁忙时重新定位工作线程。

ghOSt策略表现良好，不需要修改Snap就能与自定义内核调度程序相似。ghOSt允许快速实验和性能优化，比内核调度程序更容易。预计通过包括来自工作线程的调度提示等额外改进，可以进一步优化性能。

4.4 Google Search

本文评估了ghOSt作为CFS调度器的替代品，用于为Google搜索查询提供服务的机器。

该基准测试包括三种不同的查询类型，分别为A、B和C。A类型需要CPU和内存密集型查询，由需要时唤醒的工作线程提供服务。B类型需要访问SSD，但需要很少的计算，由短暂的工作线程提供服务。C类型需要CPU密集型负载，由长时间运行的工作线程提供服务。查询生成器在没有ghOSt的情况下在另一台机器上运行。

在数据包进入时，查询会先由服务器线程处理，然后创建子查询交给工作线程处理。一些子查询需要由特定的工作线程处理以利用数据本地性。由于这个工作负载是内存和CPU密集型的，如果查询由运行在访问数据所在的同一插槽上的工作线程处理，查询的服务速度会更快。

我们在评估ghOSt时使用的机器配备了两个AMD Zen Rome处理器，总共有256个CPU（2个插槽，每个插槽64个物理核心，每个核心有2个逻辑核心）。AMD的架构带来了新的挑战，因为它将4个物理核心（8个逻辑核心）分组成CCX（CPU核心复合体），每个CCX都有自己的L3缓存。

PAGE 12

该策略使用ghOSt的集中式模型，由一个全局代理调度所有256个CPU。全局代理首先使用sysfs生成系统拓扑模型，然后根据NUMA偏好调度线程，并尽可能优先在上次运行线程的CCX上运行线程。全局代理维护一个按线程运行时间排序的最小堆，选择已经运行时间最短的线程进行执行。线程运行到完成或被CFS线程抢占。

ghOSt的NUMA和CCX感知启发式算法只有57行代码，这得益于其灵活的事务API和使用C++标准库。当新的工作线程被创建时，它的cpumask被设置为其查询数据所在的套接字中的CPU集合。这个cpumask作为THREAD_CREATED消息的一部分发送给全局代理。当ghOSt全局代理想要在运行队列的前面运行下一个线程时，它将线程的cpumask与空闲CPU集合取交集。如果交集为空，代理跳过该线程并调度运行队列中的下一个线程，在下一次调度循环中重新访问被跳过的线程。

该策略会将每个线程分配到最近的空闲CPU目标上，以提高搜索查询性能。首先在与线程上次运行的CPU相同的L1和L2缓存域内搜索可用的CPU，如果没有空闲CPU，则扩展到CCX（L3缓存）域。如果仍然失败，则进行扇出搜索，以避免高互连CCX通信延迟导致的昂贵线程迁移成本。

本文比较了CFS和ghOSt策略在搜索基准测试中的查询延迟和吞吐量。结果显示，ghOSt的吞吐量与CFS相当，而且ghOSt的NUMA和CCX优化对于实现与CFS的相似性至关重要。在短的ghOSt策略中迭代优化NUMA和CCX位置比在内核CFS代码中进行实验更容易。每次修改ghOSt代理只需要重新启动代理进程，而修改CFS则需要安装内核并重新启动。

ghOSt策略在查询类型A和B上可以减少40-45%的尾延迟，与CFS相比，查询类型C的尾延迟相当。在进行套接字和CCX优化之前，ghOSt策略在查询类型A上的延迟几乎是CFS的两倍，而在查询类型B和C上与CFS相当。查询类型A受益于拓扑优化，查询类型B同时访问内存和SSD，而查询类型C则主要受计算限制。ghOSt策略在B和C上具有优势，因为全局代理可以快速响应整个系统容量的变化，在微秒级别上重新平衡CPU上的线程。CFS则只在毫秒级别的周期性间隔内重新平衡CPU上的线程，从而损害查询的尾延迟。

通过进一步优化策略，可以提高 ghOSt 上查询 C 的延迟。工作负载为线程分配 nice 值，以表达相对优先级顺序，这对于确保工作线程比低优先级的后台线程（例如垃圾回收）具有更高的优先级非常重要。CFS 使用这些 nice 值做出更优化的决策，初步实验表明将它们纳入 ghOSt 的策略中将使 ghOSt 在查询 C 的尾延迟方面超过 CFS。

ghOSt是我们生产车队的可行解决方案，能够为大型机器和真实工作负载进行调度，同时实现快速开发和推出调度策略。

开发内核调度程序时，写-测试-写循环包括编译内核、部署内核和运行测试。使用ghOSt，这个过程可以在一分钟内完成，而且不需要重新启动。这对于优化开发非常重要，因为重新启动会导致测试结果的变化。

我们利用实验的便利性进行了定制优化的实验，否则很难发现。例如，在Rome架构中，由于CCX内部和CCX之间的延迟变化较大，我们发现如果一个线程的首选CCX集群不可用，暂时将线程挂起100微秒而不是立即迁移到另一个CCX会更高效。

4.5 Protecting VMs from L1TF/MDS Attacks

该政策保护虚拟机免受跨超线程的推测执行攻击，如L1TF和MDS。通过确保每个物理核心只运行来自同一虚拟机的虚拟CPU（vCPU），可以减轻这些攻击。当在物理核心上调度新的虚拟机时，微架构缓冲区会被清除。

PAGE 13

为了防止跨超线程攻击，需要对物理核心进行调度，每个物理核心有两个逻辑CPU。在每个CPU模型中实现核心调度很具有挑战性，因为调度程序代码只能在当前执行的CPU上运行线程。相比之下，ghOSt代理可以通过为每个物理核心执行同步组提交来轻松地调度整个核心，即为一个核心的两个CPU发出提交，这些提交必须全部成功或全部失败。

本文介绍了一个基于每个物理核心的调度模型，其中每个核心都有两个兄弟CPU，它们将消息发布到同一个队列中。只有一个代理在物理核心上处于活动状态，它会为两个CPU做出调度决策并提交一个组事务。ghOSt策略确保运行在物理核心上的线程属于同一个虚拟机。虚拟机的线程可以占用两个兄弟中的一个，另一个兄弟运行空闲线程。

本文介绍了一种安全的虚拟机核心调度策略，该策略确保前向进展、限制尾延迟，并提供良好的平均延迟。该策略使用分区EDF方案，每个物理核心为每个线程分配保证时间来运行，限制尾延迟。多余的时间公平地分配给可运行的线程，提高平均延迟。运行队列跨越单个NUMA节点。在高系统负载下，该策略允许跨越运行队列，提供NUMA优先级。

本实验使用32个虚拟CPU和25个物理核心进行测试，比较了三种调度策略：CFS、内核安全虚拟机核心调度和ghOSt安全虚拟机核心调度。结果表明，CFS性能更好，但没有安全性；ghOSt策略可以缓解跨超线程攻击，并且在额外的上下文切换开销下表现类似于内核版本的核心调度。

5 Future Work

使用BPF回调函数可以加速ghOSt的调度，减少调度间隙。全局代理调度循环需要30微秒，有些线程只运行5-30微秒就会阻塞，导致CPU空闲。使用集成的BPF程序可以缓解这些调度间隙。

BPF程序通过多个生产者、多个消费者的环形缓冲区与用户空间通信。代理将可运行线程插入缓冲区，BPF尝试运行它们。代理可以在BPF调度线程之前撤销线程。全局代理可以为每个NUMA节点使用一个环形缓冲区，并跟踪每个线程的首选NUMA节点，以在两个环之间平衡负载。

ghOSt在集中模式下可以禁用CPU上的定时器滴答声，以避免在虚拟机工作负载中产生昂贵的VM退出。在经典的每CPU调度程序中，滴答声会触发调度程序，以确保所有虚拟机的轮询抢占。不幸的是，这些滴答声会导致VM退出到主机内核上下文。

PAGE 14

全局代理不需要CPU时钟中断，可以减少客户端的抖动。这种优化在CFS中不可能实现，最接近的选项是启用CONFIG_NO_HZ_FULL，但只有在CPU上没有超过一个可运行线程时才会禁用时钟中断。

6 Related Work

我们简要讨论了与ghOSt相关的其他先前工作。

Scheduler Activations是一个API，允许应用程序协调由内核分配给它的CPU的调度。与ghOSt不同，Scheduler Activations允许应用程序对CPU的分配或移除做出反应，但不允许将CPU分配给应用程序。用户级线程库和调度器可以在内核线程之上多路复用用户空间上下文，但不能控制内核线程何时何地运行。

最近的研究探索了将调度和应用程序从主机转移到SmartNIC的正确策略和机制。 ghOSt的共享内存队列和事务API旨在与新的一致性互连技术（如CXL）无缝配合，允许ghOSt部分或全部地卸载到SmartNIC。

最近出现了一种趋势，即将内核组件转移到用户空间，类似于微内核。DPDK、IX和Snap将网络驱动程序和堆栈转移到用户空间，SPDK、ReFlex、FUSE和DashFS将存储和文件系统操作转移到用户空间。Linux用户空间I/O（UIO）促进了内核驱动程序的转移。甚至提出了一种新的CPU设计来加速微内核。ghOSt延续了这一趋势。

之前的研究表明将调度器移至用户空间。Stoess开发了一个层次化的用户级调度器，但需要修改应用程序来实现自定义调度策略。与之不同的是，ghOSt不需要修改应用程序。

Ford和Susarla的CPU Inheritance Scheduling是一个用户级调度系统，通过线程捐赠运行时间来进行调度。每个CPU的根调度程序将其运行时间捐赠给另一个线程，可以是调度程序，也可以是应用程序线程。这些调度程序接收有关线程和系统事件的消息。系统的调度程序处理机制：执行捐赠和发送消息。与ghOSt不同，每个调度程序一次只能调度一个CPU。

7 Conclusion

ghOSt是一个新的平台，用于评估和实现现代数据中心的线程调度策略。它将调度器开发从单片内核实现的领域转变为更灵活的用户空间设置，允许应用广泛的编程语言和库。ghOSt允许我们快速开发生产软件的策略，快速测试和迭代，与现有调度器相比具有竞争性的性能。我们开源ghOSt，作为未来用户驱动资源管理的研究基础。

Acknowledgments

感谢Eric Brewer、David Culler、Hank Levy、Amin Vahdat、Kostis Kaffes、Adam Belay、Josh Fried、David Mazières、Irene Zhang、Google Systems Infrastructure、Stanford Platform Lab和匿名的SOSP审稿人的有益反馈。Christos Kozyrakis得到了Stanford Platform Lab的支持。
