https://zhuanlan.zhihu.com/p/414281503

我觉得可以直接看这个中文的阅读记录，我想应该是很好玩的



# 有道速读的翻译

Abstract

数据中心应用需要操作系统在微秒级别的尾延迟和高请求率下运行，并且大多数应用程序处理具有多个时间尺度的高方差负载。目前最好的解决方案是内核旁路网络，但这种方法浪费了CPU。即使在平均负载较低的情况下，也必须为预期峰值负载分配足够的核心。

Shenango是一个高效的系统，能够在应用程序之间实现细粒度的核心重新分配，以提高CPU效率。它通过一种高效的算法和一个专用核心上运行的IOKernel组件实现快速的重新分配速率。在处理延迟敏感的应用程序时，Shenango能够实现与ZygOS相当的尾延迟和吞吐量，但可以线性地将延迟敏感的应用程序的吞吐量与批处理应用程序的吞吐量进行交换，从而大大提高CPU效率。

1 Introduction

数据中心应用需要支持高请求率和微秒级尾延迟，但当前操作系统和网络堆栈的延迟在毫秒级别。高速网络可以提供几微秒的往返时间，但需要改进操作系统和网络堆栈以实现更低的延迟。

随着摩尔定律的放缓和网络速率的提高，CPU效率变得至关重要。在大规模数据中心中，即使是CPU效率的微小改进也可以节省数百万美元。因此，数据中心运营商通常会利用未被延迟敏感任务使用的核心来运行批处理应用程序，以保持CPU利用率高。例如，微软必应在超过90,000台服务器上同时运行延迟敏感和批处理作业，而谷歌计算集群中的中位数机器运行八个应用程序。

现有系统在需要保持微秒级尾延迟的同时，很难实现高CPU效率。Linux只能在保持低CPU利用率的情况下支持微秒级延迟，需要有足够的空闲核心来快速处理请求。另外，绕过内核调度程序的内核绕过网络堆栈（如ZygOS）可以以更高的吞吐量支持微秒级延迟，但仍会浪费大量CPU周期。此外，它们缺乏快速重新分配核心的机制，因此必须配置足够的核心来处理峰值负载。

数据中心工作负载的突发到达模式使得低尾延迟和高CPU效率之间的紧张关系更加严重。这种变化需要服务器始终保持额外的核心空闲，以便在突发期间保持低尾延迟。

现今系统的低效率和尾延迟问题是由于系统软件被调整为毫秒级I/O（例如磁盘），导致我们不得不浪费核心来维持微秒级延迟。现有的调度器只能在粗粒度（Linux每四毫秒，Arachne和IX每50-100毫秒）上做出线程平衡和核心分配决策，无法快速应对负载不平衡。谷歌最近的一篇论文也提出了类似的观点。

Shenango是一个系统，旨在实现三个目标：（1）为数据中心应用程序提供微秒级的端到端尾延迟和高吞吐量；（2）在多核机器上实现高效的应用程序打包；（3）通过同步I/O和标准编程抽象（如轻量级线程和阻塞TCP网络套接字），提高应用程序开发人员的生产力。

PAGE 3

Shenango通过每5微秒重新分配核心来解决应用程序之间的核心分配问题。它提出了两个关键想法：一是**通过算法确定应用程序何时需要额外的核心**，二是**将一个忙碌的核心分配给IOKernel**，以便将数据包分配给应用程序并分配核心。应用程序在用户级运行时中运行，与IOKernel通信以便进行核心分配。

Shenango是一个高效的内核旁路网络堆栈，可以实现高吞吐量和低尾延迟。与ZygOS相比，Shenango的CPU效率更高，可以在请求率低于峰值负载时线性交换memcached吞吐量和批处理应用吞吐量。Shenango是第一个可以在微秒级负载爆发期间同时多路复用核心和保持低尾延迟的系统。它可以在极端负载从10万到500万请求每秒的情况下，快速反应以保持99.9%的尾延迟低于125微秒。

2 The Case Against Slow Core Allocators

本文解释了为什么毫秒级别的核心分配器无法在处理微秒级别的请求时保持高CPU效率。CPU效率定义为花费在应用程序级别工作的周期比例，而不是忙等待、上下文切换、数据包处理或其他系统软件开销。

现代数据中心应用程序在多个时间尺度上经历请求率和服务时间的变化。为了在面对这些波动时提供低延迟，大多数内核绕过网络堆栈（包括ZygOS）会**静态分配核心**以应对峰值负载，从而浪费了大量的繁忙轮询周期。最近，一些努力（如IX和Arachne）引入了用户级核心分配器，可以在50-100毫秒的间隔内调整核心分配。类似地，Linux主要通过毫秒级定时器滴答来重新平衡任务在核心之间的分配。不幸的是，所有这些系统调整核心的速度都太慢，无法高效处理微秒级请求。

通过模拟器，我们确定了一个调整核心的分配器的CPU效率的保守上限。模拟器模拟了一个M/M/n/FCFS排队系统，并通过试错法确定了为了维持给定的负载水平的尾延迟限制所需的最小核心数。我们假设到达过程为泊松过程，服务时间呈指数分布，平均为10微秒，99.9％分位数的延迟限制为100微秒。这些假设允许我们计算出最佳情况下的CPU效率，无论使用什么核心分配算法。

本文介绍了一个模拟实验，展示了负载和CPU效率之间的关系。即使没有网络或系统软件开销，空闲核心也必须被保留以吸收负载突发，导致CPU效率下降。 Shenango在上下文切换、同步等方面产生了实际开销，但仍然比理论上限高效。最理想的系统将为每个请求启动一个核心，实现完美效率。

PAGE 4

慢速核心分配器在实践中可能表现不如理论上限，因为CPU效率会更低，如果服务时间变化更大或尾延迟要求更紧，那么效率会更低。如果平均请求率在调整间隔期间发生变化，延迟会增加，直到添加更多核心；准确预测核心数量和性能之间的关系在毫秒级间隔内非常困难。Shenango的快速核心分配速率使其能够克服这些问题。

3 Challenges and Approach

Shenango的目标是通过尽可能少地分配核心给每个应用程序来优化CPU效率，同时避免计算拥塞的情况。这样可以释放出未充分利用的核心供其他应用程序使用，同时保持尾延迟在可控范围内。

现代服务经常面临非常高的请求率，每秒单个服务器处理数百万个数据包，而核心分配开销使得每个请求都重新分配核心不可行。Shenango通过每5微秒检测负载变化并每秒调整核心分配60,000次来近似这个理想状态。这样短的调整间隔需要新的负载估计方法。本文将详细讨论这些挑战。

**核心分配会带来开销**，实际上，核心重新分配的速度受到重新分配开销的限制。现有系统的开销太大，无法在微秒级别实现核心重新分配。Arachne需要29微秒的延迟来重新分配核心，IX需要数百微秒，因为它必须更新NIC规则以将数据包转发到核心。

Shenango难以估计所需的核心数，因为应用程序级指标无法应用于微秒级间隔。它旨在估计瞬时负载，但这是非常困难的。网络请求提供了一种负载来源，但应用程序本身也可以独立生成线程。

3.1 Shenango ’s Approach

Shenango解决这些挑战的两个关键点是：首先，Shenango将线程和数据包排队延迟视为计算拥塞的信号，并引入了一种高效的拥塞检测算法，利用这些信号来决定应用程序是否需要更多的核心。这个算法需要对每个应用程序的线程和数据包队列进行细粒度、高频率的可见性。因此，Shenango的第二个关键点是将一个**单一的忙旋转核心专门分配给一个名为IOKernel的集中式软件实体**。IOKernel进程以root权限运行，作为应用程序和NIC硬件队列之间的中间人。通过忙旋转，IOKernel可以以微秒级的时间检查线程和数据包队列，以协调核心分配。此外，它可以提供低延迟的网络访问，并在软件中实现数据包的转发，允许在重新分配核心时快速重新配置数据包转发规则。结果是，核心重新分配仅需5.9微秒完成，并且需要不到两微秒的IOKernel计算时间来协调。这些开销支持了足够快的核心分配速率，以适应负载的变化，并快速纠正拥塞检测算法的任何错误预测。

应用程序逻辑在每个应用程序运行时中运行，**通过共享内存与IOKernel进行通信**。每个运行时都是不可信的，并负责提供有用的编程抽象，包括线程、互斥锁、条件变量和网络套接字。应用程序通过链接Shenango运行时库，使得类似内核的函数可以在其地址空间内运行。

运行时会创建多个内核线程，每个线程都有一个本地运行队列，最多使用可用的核心数。

PAGE 5

Shenango是一个在Linux环境下运行的应用程序逻辑，使用轻量级用户级线程和工作窃取技术来平衡核心的工作负载。IOKernel可以管理一部分核心，而Linux调度程序可以管理其他核心。

4 IOKernel

IOKernel在专用核心上运行，主要执行两个功能。

在任何给定的时间，操作系统决定为每个应用程序分配多少个核心（§4.1.1），以及为每个应用程序分配哪些核心（§4.1.2）。

该系统处理所有网络I/O，绕过内核。在接收路径上，它直接轮询NIC接收队列，并将每个传入的数据包放入应用程序的一个核心的共享内存队列中。在传输路径上，它轮询每个运行时的数据包出口队列，并将数据包转发到NIC。

4.1 Core Allocation

IOKernel需要快速做出核心分配决策，因为它花费在核心分配上的时间会减少吞吐量。为了简化，IOKernel将其两个决策分开，大多数情况下，它首先决定是否应该为应用程序授予额外的核心，然后再决定授予哪个核心。

4.1.1 Number of cores per application

应用程序运行时有保证的核心数和可突发的核心数。保证的核心数不会被抢占，但可用核心数可以超过保证的核心数。当有额外的核心可用时，可以分配为可突发的核心，允许繁忙的运行时暂时超过其保证的核心限制。

IOKernel决定为每个运行时分配多少个核心时，目标是尽可能减少每个运行时的核心数量，同时避免计算拥塞。IOKernel依靠运行时kthreads自愿放弃不需要的核心来确定运行时是否有多余的核心。当kthread找不到任何工作要做时，它会放弃核心并通知IOKernel（我们称之为停车）。IOKernel也可以随时抢占可爆发的核心，强制它们立即停车。

IOKernel利用其独特的视角通过监视活动k线程的队列占用情况来检测即将发生的计算拥塞。当一个运行时没有分配的核心时，IOKernel立即为其分配一个核心。为了监测拥塞的活动运行时，IOKernel每隔5微秒调用一次拥塞检测算法（算法1）。

拥塞检测算法根据排队的线程和排队的入站数据包两个负载来源来确定运行时是否过载。如果在检测算法的连续两次运行中发现任何项目存在于队列中，则表示数据包或线程排队至少5微秒。因为排队的数据包或线程代表可以在另一个核心上并行处理的工作，所以运行时被认为是“拥塞的”，IOKernel会为其提供额外的一个核心。我们发现排队持续时间比队列长度更可靠，因为使用队列长度需要对不同请求持续时间进行仔细调整阈值参数。

将队列实现为环形缓冲区可以实现简单高效的检测机制。检测队列中连续两个时间间隔存在一个项目只需比较当前头指针和上一次迭代的尾指针。运行时将此状态暴露给IOKernel，每个kthread共享一条缓存线。

PAGE 6

核心分配具有振荡行为，可以在每次迭代中添加和停放核心。现代CPU具有足够高效的上下文切换能力，可以处理IOKernel产生的核心重新分配速率。在第7.3节中，我们评估了不同核心分配间隔对尾延迟和CPU效率的影响。

4.1.2 Which cores for each application

IOKernel在决定授予应用程序哪个核心时，考虑了三个因素。

Intel的HyperThreads可以在同一物理核心上运行两个硬件线程。如果来自同一应用程序的超线程在同一物理核心上运行，则可以从缓存局部性中受益；如果来自不同应用程序的超线程共享同一物理核心，则可能会争夺缓存空间并降低彼此的性能。因此，IOKernel倾向于将同一物理核心上的超线程授予同一应用程序。

应用程序的状态如果已经存在于核心的L1/L2缓存中，可以避免许多耗时的缓存未命中。由于超线程共享相同的缓存资源，将应用程序授予已运行核心的超线程对将产生良好的缓存局部性。此外，通过在最近运行的核心上运行，应用程序可能会获得缓存局部性的好处。因此，IO内核跟踪运行时的当前和过去的核心分配。

IOKernel总是优先分配空闲核心，而不是抢占正在忙碌的核心，以减少延迟和浪费。

IOKernel的核心选择算法（算法2）考虑了三个因素。只有当核心处于**空闲状态**（第2行），或者**没有空闲核心且正在使用核心的应用程序正在爆发**（使用超过其保证的核心数量）时，核心才有资格被分配（函数CAN BE ALLOCATED）（第4行）。在符合条件的核心中，选择算法SELECT CORE首先尝试分配**应用程序当前正在使用的核心的超线程对**（第9-12行）。接下来，它尝试分配该应用程序**最近使用但不再使用的核心**（第13-15行）。最后，算法**选择任何空闲核心（如果存在）**，或者从爆发应用程序中选择一个随机核心。

PAGE 7

IOKernel选择一个核心给应用程序，并选择一个最近在该核心上运行的**kthread**来唤醒和运行。如果没有可用的kthread，则选择最长时间停留的kthread，留下其他kthread停留，以防它们最近运行的核心变得可用。为了缓存局部性，IOKernel首先尝试选择最近在该核心上运行的kthread。

SELECT CORE ( APP ) 的运行时间与 APP 的活动核心数成线性关系（它检查每个活动核心是否有可用的超线程）。拥塞检测算法在**一次遍历中最多可能调用 SELECT CORE 一次**，而活动应用程序的活动核心总数不会超过系统中的核心数。因此，调用检测算法的总成本与核心的总数成线性关系。

4.2 Dataplane

IOKernel忙等待，不断轮询传入的NIC数据包队列和传出的应用程序数据包队列。

Shenango使用Packet steering技术，通过IOKernel跟踪每个运行时的核心，将传入的数据包直接传递到运行相应运行时的核心。**每个运行时都有自己的IP和MAC地址，IOKernel通过哈希表查找MAC地址来确定运行时**，并使用**RSS哈希选择核心**，将数据包排队到该核心的入站数据包队列中。Shenango可以通过技术如Intel的Flow Director或FlexNIC进一步优化数据包转发。

为了避免在系统中轮询多个出口队列以查找要传输的数据包，IOKernel跟踪活动的kthreads，只轮询与活动kthreads相对应的出口运行时数据包队列，从而使轮询出口队列的CPU开销随着系统中核心数的增加而扩展。

5 Runtime

Shenango是一个可编程的运行时系统，提供高级抽象，如阻塞TCP网络套接字和轻量级线程。它的设计可以扩展到数千个uthreads，每个uthread都能够执行任意计算，并插入同步I/O操作。与之前的内核绕过网络堆栈相比，Shenango不会为了性能而牺牲功能，而是提供了与Berkeley Sockets相似的API，使开发人员可以使用更自由的编程模型。

**该运行时类似于库操作系统，与每个应用程序的地址空间链接**。初始化后，应用程序只与Linux内核交互以分配内存。运行时提供了内核绕过的替代方案，以避免阻塞内核操作。此外，内存和CPU使用情况可以完全归属于每个应用程序，因为内核不再代表它们执行这些请求。

运行时在**应用程序内部进行调度，动态分配给它的核心**。在初始化时，运行时向IOKernel注册其kthreads，并为网络数据包队列建立共享内存区域。每次IOKernel分配一个核心时，它会唤醒运行时的一个kthread并将其绑定到该特定核心。

该运行时采用基于线程的运行队列和工作窃取的结构，类似于Go，与Arachne的工作共享模型不同。通过**细粒度的工作窃取**，可以降低尾延迟，特别适用于服务时间变化的工作负载。由于只有本地kthread可以附加到其运行队列，因此可以在不锁定的情况下执行uthread唤醒。

我们的运行时采用了运行到完成的策略，允许uthreads在大多数情况下无中断地运行，直到它们自愿放弃。这种策略可以进一步减少轻尾请求模式下的尾延迟。当uthread放弃时，必要的寄存器状态会被保存在堆栈上，以便稍后恢复执行。当放弃是协作的时候，我们可以保存更少的寄存器状态。

PAGE 8

函数调用边界允许覆盖一些通用寄存器以及所有向量和浮点状态。但是，如果IO内核回收了一个核心，任何uthread都可能被抢占；在这种情况下，**必须保存所有寄存器状态**。

调度程序在进行yield后，会先检查本地运行队列，如果为空且没有要处理的传入数据包或过期计时器，则进行工作窃取。它首先检查核心的超线程兄弟以利用缓存局部性。如果失败，则尝试从随机的kthread中窃取。最后，调度程序遍历所有活动的kthread。如果所有尝试都失败，则调度程序将kthread停放，将其核心归还给IOKernel。

运行时负责为应用程序提供所有网络功能，包括UDP和TCP协议处理。每个k线程在uthread让出或本地运行队列为空时，检查其入口数据包队列以处理新的数据包。与以前的系统不同，k线程还可以从远程入口数据包队列中窃取数据包。这与ZygOS不同，ZygOS可以窃取TCP套接字层以上的应用程序级工作，但必须保持数据包的流一致哈希。因此，这种窃取以及IOKernel进行的数据包定向调整可能会导致短时间内的数据包重排序。

本文介绍了一些高效的技术来重新排序数据包，同时提供了一种低开销的机制来在传输层重新组装数据包序列。这种重新排序需要获取每个套接字的锁，但由于同一流的数据包通常在短时间内到达同一核心，因此缓存局部性得到保留，获取锁的开销很小。

放宽排序要求和违反流一致性哈希算法可以带来显著的优势。ZygOS需要在同一核上发送和接收来自同一流的数据包，因此需要昂贵的IPI来确保及时处理未处理的入站数据包，并确保出站处理在同一核上进行。相比之下，Shenango的方法可以更细粒度地负载均衡网络流处理，从而在不平衡的工作负载下获得更好的性能。

早期版本的运行时尝试支持零拷贝网络，但发现这种方法有严重的缺点。首先，它需要API更改，破坏了与Berkeley Sockets的兼容性。其次，我们惊讶地发现它对性能有负面影响。进一步调查后，我们发现我们的IOKernel的吞吐量对驻留缓冲区的数量敏感，因为DDIO（Intel技术，将数据包直接推送到LLC）对可以被数据包占用的最大缓存行数有限制。当超过该限制时，数据包数据被推送到RAM，大大增加了访问延迟。通过复制有效载荷，我们可以鼓励DDIO重复使用相同的缓冲区，从而保持在其缓存占用阈值内。这与“泄漏DMA”问题相似。

应用程序可能会破坏其运行时网络堆栈，因此我们假设安全验证（例如带宽限制和网络虚拟化）将以与虚拟机客户机内核完全相同的方式在带外高效处理。

6 Implementation

Shenango是一个基于用户空间网络协议栈的系统，由IOKernel和runtime两部分组成，支持64位x86平台，使用Intel DPDK进行快速访问NIC队列，整个系统在未修改的Linux环境下运行。实现代码量为2,244 LOC和6,155 LOC，还包括4,762 LOC的自定义库例程。

6.1 IOKernel Implementation

Shenango使用Linux内核机制来将线程固定到核心并在IOKernel和运行时之间进行通信。IOKernel通过System-V共享内存段传递数据，运行时设置了一系列描述符环队列，包括入站数据包队列、出站数据包队列和单独的出站命令队列。它还指定了一部分映射内存用于传出网络缓冲区。我们目前将所有入站数据包缓冲区放置在一个只读区域中，与所有运行时共享。将来，我们计划使用NIC硬件过滤器来维护单独的缓冲区，以隔离数据包。

IOKernel使用sched setaffinity将运行时kthread分配给特定的核心。IOKernel与每个kthread维护一个共享的eventfd文件描述符。当一个kthread找不到更多的uthreads来运行时，它通过命令队列消息通知IOKernel它正在停车，然后通过对其eventfd执行阻塞读取来停车。要解除kthread的停车状态，IOKernel只需向eventfd写入一个值。当IOKernel需要重新分配核心时，它通过tgkill系统调用向目标kthread发送SIGUSR1信号来抢占运行时kthread。这会促使kthread停车。一个恶意的kthread可能会拒绝在收到信号后停车。虽然我们尚未实施缓解策略，但IOKernel可以等待几微秒，然后将有问题的kthread迁移到由Linux调度程序复用的共享核心上，以避免影响其他运行时。

6.2 Runtime Implementation

PAGE 9

该运行时支持轻量级线程、互斥锁、条件变量、读-复制-更新（RCU）、高分辨率定时器和同步TCP和UDP套接字。它使用现有的Linux原语，通过mmap分配内存，通过pthread create()调用创建kthreads，并通过共享内存、eventfd文件描述符和信号与IOKernel交互。该TCP堆栈是根据RFC [36]从头开始实现的，与Linux和ZygOS的TCP堆栈兼容，包括流量控制和快速重传，但省略了拥塞控制。

为了提高内存分配性能，运行时使用每个kthread缓存，特别是在分配线程堆栈和网络数据包缓冲区时。运行时提供了一个RCU子系统，以支持对只读数据结构的高效访问。运行时在每个kthread重新调度后检测到一个静止期，从而可以释放任何过时的RCU对象。在内部，RCU用于ARP表以及TCP和UDP套接字表。

Shenango提供了C++和Rust的绑定，支持lambda和闭包。大部分绑定都是对底层C库的薄包装。但是，他们的uthread支持利用了一种独特的优化，通过扩展Shenango的spawn函数，在每个uthread的堆栈底部预留空间来存储跳板数据，避免了额外的分配。

当IOKernel收到SIGUSR1信号时，Linux内核会将CPU状态保存到线程堆栈上的陷阱帧中，并调用运行时安装的信号处理程序。信号处理程序立即转移到调度程序上下文并停止，将被抢占的uthread放回运行队列。正在运行的uthread最终可能会被另一个kthread窃取或在重新获得核心的情况下恢复在同一个kthread上。

在运行时执行的某些关键部分，通过增加线程本地计数器来推迟抢占信号。这些部分包括整个调度器上下文、RCU和自旋锁关键部分，以及访问每个kthread状态的代码区域。支持对活动uthreads的抢占存在一些挑战。如果线程上下文开始在不同的kthread上执行，线程本地存储（TLS）的指针可能会变得过时。不幸的是，gcc没有提供禁用缓存这些地址的方法。据我们所知，微软的C++编译器是唯一支持此功能的编译器。作为一种解决方法，我们使用自己的TLS机制来处理在调度器上下文之外访问的每个kthread数据结构，并且我们目前要求应用程序在访问线程本地变量（包括glibc的malloc和free）时禁用抢占。我们正在考虑扩展运行时以支持每个uthread的TLS，以减轻开发人员的负担。然而，为了避免在生成uthreads时产生更高的初始化开销，TLS数据段必须保持较小。

7 Evaluation

评估Shenango的目标是回答以下问题。

Shenango和其他系统在不同工作负载和服务时间分布下的延迟和CPU效率如何比较？

沈纳哥能够很好地应对突然的负荷增加。

Shenango的个体机制对其观察到的性能有何贡献？

实验使用了一台双插槽服务器，配备12核Intel Xeon E5-2650v4 CPU、64GB RAM和10Gbits/s Intel 82599ES NIC。为了减少抖动，关闭了TurboBoost、C-states和CPU频率缩放。通过Mellanox SX1024交换机和Mellanox ConnectX-3 Pro NICs连接了六台四核机器。使用Ubuntu 18.04和内核版本4.15.0。为了与之前的结果保持一致，禁用了Meltdown的内核缓解措施。

Shenango与Arachne、ZygOS和Linux进行了比较。Arachne是一种先进的用户级线程系统，通过引入用户级核心分配器，可以在毫秒级时间尺度上调整分配给每个应用程序的核心，从而实现更好的尾延迟和CPU效率。然而，Arachne没有网络堆栈集成，应用程序通常依赖于Linux内核系统调用进行网络I/O。ZygOS是一种先进的内核绕过网络堆栈，构建在IX之上，通过在核心之间进行应用级工作的细粒度负载均衡，实现更好的尾延迟。然而，它不支持线程，而是要求开发人员采用限制性的事件驱动API，并且只能在一组固定的静态分配的核心上运行。最后，Linux是这些系统中实际上部署最广泛的系统，但其性能受到内核开销的限制。表1总结了Shenango与这三个系统之间的显著差异。

PAGE 10

Arachne使用了最新的源代码，发现默认的负载因子1.5效果最好。ZygOS使用了最新的源代码，但发现与最近的内核不稳定，因此使用了Ubuntu 16.04和内核版本4.11.0。

在Linux系统中，我们使用了之前的工作并投入了大量的努力来找到最佳配置。然而，Linux的性能经常不稳定，这给测量工作带来了挑战。我们发现了性能滞后的迹象，即使配置相同，测量运行也会收敛到不同的值。增加活动流的数量可以解决这个问题，因为这样可以实现更均匀的RSS散列。我们使用SCHED IDLE来运行批处理任务（这是Linux的一种调度策略，用于非常低优先级的后台作业），尽管我们发现与使用最低正常调度器优先级（niceness 19）相比，这并没有显著提高性能。

本文评估了memcached（v1.5.6）和几个新的Shenango应用程序，以测量不同的负载模式。

作者使用了闭包和移动语义等语言特性，实现了一个模拟计算密集型应用的自旋服务器和一个能够生成精确时间请求模式的负载生成器。这两个应用程序共计 1,366 行代码。作者还比较了 ZygOS 和 Linux 自旋服务器的变体，并为 Arachne 实现了自己的自旋服务器。

为了支持批处理应用程序，我们在Shenango上实现了一个pthread shim层，使其能够运行整个PARSEC套件，我们使用PARSEC的swaptions基准测试进行实验。最后，我们将gdnsd DNS服务器移植到Shenango上，以展示其UDP支持。

使用开环泊松过程模拟数据包到达。实验测量吞吐量和99.9%尾部响应延迟。除非另有说明，所有实验都使用我们的Rust负载生成应用程序通过TCP生成负载。

7.1 CPU Efficiency and Latency

本文评估了memcached、spin-server和gdnsd的CPU效率和延迟。使用6个客户端服务器生成负载，每个客户端使用200个持久连接。逐渐增加负载并测量每个负载的持续时间，以便突发负载只来自泊松到达过程。

为了与只支持16个超线程的ZygOS进行公平比较，我们将所有系统限制为仅使用16个超线程（8个核心）。Shenango必须将一个核心（2个超线程）用于运行IOKernel，因此应用程序可用的超线程数量减少了两个。Arachne必须将一个超线程用于核心仲裁器。除了ZygOS外，我们还运行swaptions，利用未使用的周期进行低优先级批处理工作。对于ZygOS，我们保留所有16个超线程用于低延迟应用程序，以实现最大吞吐量。

本文介绍了在Shenango上运行Memcached的性能测试。使用USR工作负载，其中99.8%的请求为GET请求，0.2%的请求为SET请求。限制Memcached最多使用12个超线程，可以获得最佳性能。通过增加负载，测试了99.9th百分位延迟、中位数延迟和批处理应用程序的吞吐量的变化。

PAGE 11

Shenango可以处理超过500万个请求每秒，同时保持中位数响应时间为37微秒和99.9百分位响应时间为93微秒。尽管在所有16个超线程上进行繁忙轮询，ZygOS仅在每秒处理400万个请求时保持类似的响应时间。然而，ZygOS可以扩展以支持更高的吞吐量，但会有较高的延迟惩罚。Shenango实现较低的吞吐量是因为在memcached的非常低的服务时间（<2微秒）下，IOKernel成为瓶颈。我们在第8节讨论了进一步扩展IOKernel的选项。对于所有其他系统，memcached受到CPU的限制。

本文研究了Linux、Arachne、Shenango和ZygOS等系统在memcached中的性能表现。结果显示，Linux和Arachne在没有批处理工作时，可以达到约800,000个请求每秒，但是在有批处理工作时，Linux的延迟明显下降，尤其是在99.9百分位数上。Shenango和ZygOS通过内核绕过实现了更低的延迟和更高的吞吐量。

Shenango在批处理应用的吞吐量方面表现优异，除了最低负载外。随着负载的增加，Shenango的批处理吞吐量线性下降，然后在任务被限制为仅剩下两个超线程时达到平稳状态。然而，随着负载的增加，memcached的吞吐量仍然增加，因为Shenango在接近峰值负载时变得更加高效。

Shenango相比之前的系统具有更好的性能，可以实现与ZygOS相似的尾延迟，同时为批处理工作节省更多的周期，尽管为IOKernel保留了两个超线程。

为了评估Shenango在批处理应用程序存在的服务时间变异性下的处理能力，我们使用三种服务时间分布来运行我们的自旋服务器，每种分布的平均时间为10微秒：常数分布，其中所有请求花费相等的时间；指数分布；双峰分布，其中90%的请求花费5微秒，10%的请求花费55微秒。

图4显示了在不同负载下，自旋服务器的99.9th百分位延迟和批处理吞吐量的结果。所有系统都无法达到M/G/16/FCFS模拟的理论最大吞吐量，这是由于包处理等开销所致。与ZygOS相比，Shenango在自旋服务器上实现了稍高的吞吐量，尽管Shenango的16个超线程中有两个专用于运行IO内核。Shenango的尾延迟与ZygOS相似，但由于ZygOS必须为自旋服务器分配所有核心以实现峰值吞吐量，因此它无法实现任何批处理吞吐量。

Linux的尾延迟在99.9%的情况下变化很大，即使在低负载下也可能达到几毫秒。Arachne比Linux实现了更高的吞吐量，证明了为应用程序提供独占核心的好处。令人惊讶的是，我们观察到Arachne在最低负载下的尾延迟略高于中等负载。我们怀疑这是由于对核心需求的错误估计造成的。每次只分配少量核心，最多可达50毫秒，可能导致许多请求的延迟很高，特别是在低负载时，分配给额外负载的核心很少。我们还发现，将Arachne的核心分配间隔减小到1毫秒或100微秒，对于旋转服务器和批处理应用程序的性能来说，效果相似或更差，这表明Arachne的负载估计机制对于小的核心分配间隔调整不够精确。相比之下，在这个实验中，Shenango每秒重新分配核心多达60,000次，使其能够快速适应负载突增，并保持更低的尾延迟，同时将未使用的周期分配给批处理应用程序。

PAGE 12

我们通过同时运行gdnsd和swaptions来评估UDP性能，对于Linux和Shenango，我们没有将gdnsd移植到ZygOS或Arachne。Linux gdnsd可以在开始丢包之前每秒处理高达900,000个请求，中位数延迟为41微秒，99.9th百分位延迟为亚毫秒级。Shenango gdnsd可以扩展到每秒处理5.7百万个请求（提高了6.33倍），中位数延迟为36微秒，99.9th百分位延迟为73微秒。由于空间限制，我们省略了图表。

7.2 Resilience to Bursts in Load

本实验通过生成1微秒的虚假工作的TCP请求，测量突然负载增加对尾延迟的影响。我们提供了一个基准负载，每秒100,000个请求，持续一秒钟，然后立即增加到一个升高的速率。在新速率下再过一秒钟后，负载回到基准速率。任何未使用的核心都分配给批处理，保持总CPU利用率为100%。

本文比较了Arachne、Shenango、Linux和ZygOS的性能表现。结果显示，Arachne可以处理高达100万个请求每秒的负载，但由于核心分配速度较慢，会导致积压请求和毫秒级的尾延迟。相比之下，Shenango的反应速度非常快，即使处理从10万到500万个请求每秒的极端负载转移，也几乎没有额外的尾延迟。Linux在低负载下也有毫秒级的尾延迟，而ZygOS无法调整核心分配。

7.3 Microbenchmarks

本文评估了Shenango的各个组件的微基准测试。

Shenango是一个依赖于高效线程调度的线程库，它在常见线程操作的延迟方面优于Linux pthreads、Go和Arachne。这是因为Shenango具有预分配的堆栈、无锁唤醒和避免保存可以安全被破坏的寄存器的特点。在Go中，互斥锁稍微快一些，因为它的编译器可以内联它们。

PAGE 13

本文评估了网络堆栈和核心分配的开销。通过简单的C/C++ UDP回显基准测试，我们评估了网络堆栈的基准延迟和唤醒和抢占核心的开销。结果显示，运行时和IOKernel对于使用DPDK的原始数据包增加了很少的延迟。然而，唤醒睡眠的kthreads和抢占运行的kthreads确实会产生一些开销，由于使用了Linux系统调用。虽然我们惊喜地发现这些Linux机制的开销是可以接受的，但我们相信它们在未来可以被减少。

Shenango可以在任何核心上执行数据包处理，有效地实现负载均衡。与ZygOS相比，Shenango的细粒度工作窃取成本较低，且在不平衡的工作负载下仍能保持良好的性能。在1200个连接下，Shenango的数据包乱序率不到0.07％，在24个连接下，这个百分比会在中等负载下增加，但仍低于3％。这意味着应用程序只需花费不到0.5％的时间重新排序数据包。

Shenango的一个主要优势是能够在微秒级别对核心分配进行调整。研究表明，为了保持低尾延迟，需要频繁进行核心分配调整。然而，这种频繁的重新分配会影响CPU效率，导致批处理应用每秒执行的操作减少了6%。尽管如此，我们认为这种效率的提升并不值得至少150微秒的尾延迟增加。我们没有使用更小的间隔，因为在更快的速率下，延迟只有轻微的改善，但会浪费更多的线程资源。

8 Discussion

IOKernel支持高达6.5百万个数据包的传输速率，足以饱和10 Gbits/s NIC或40 Gbits/s NIC。评估Shenango时未考虑多插槽、NUMA机器，可以考虑在每个插槽上运行一个IOKernel实例。

PAGE 14

每个IOKernel实例可以与其他实例交换消息，从而实现粗粒度的负载均衡。这种设计可以进一步扩展我们的IOKernel。我们观察到，大部分IOKernel的开销是在转发数据包而不是在协调核心分配方面。因此，我们还计划探索硬件卸载，例如新的网络接口卡设计，可以有效地向IOKernel暴露有关排队积压的信息。

9 Related Work

两级调度：第一级空间调度器分配核心给应用程序，第二级调度器处理分配给核心的线程。调度激活提供了一个内核机制来实现两级调度。Shenango通过将第一级调度器直接与NIC结合来引入了一种新的两级调度方法。

有一些系统可以在一个或多个核心上多路复用用户空间线程，例如Capriccio、Lithe、Intel的TBB、µ Threads、Arachne和Go运行时。Shenango的运行时从这些先前的工作中借鉴了许多技术，包括工作窃取。然而，据我们所知，没有先前的系统设计能够容忍以微秒为粒度的核心分配和撤销。

动态资源分配：以往的系统使用资源控制器来监测性能指标、利用率或内部队列长度来分配线程或核心，但这些指标收集的时间太长，无法管理尾延迟。此外，利用核心利用率来估计核心需求只适用于核心即使在空闲或忙碌自旋时仍分配给应用程序的系统，这种方法浪费CPU周期。

为了减少尾延迟，提出了几种调度优化方法，如Heracles、Elfen Scheduling和Tail Control。未来，我们有兴趣探索将这些技术与Shenango集成的方法。

许多系统通过绕过内核来实现低延迟网络，使用RDMA、SR-IOV或DPDK、netmap等库。这些系统包括MICA、IX、Arrakis、mTCP、Sandstorm、FaRM、HERD、RAMCloud、SoftNIC、ZygOS、Shinjuku和eRPC。其中，IX和eRPC以批处理方式处理数据包，对于服务时间短、均匀且连接众多的工作负载，可能提供更高的吞吐量。ZygOS与Shenango最相似，它在IX的基础上添加了工作窃取以改善应用程序内的负载均衡。然而，这些系统都不能以细粒度动态重新分配核心给应用程序。它们要么静态地将核心分区给应用程序，要么使用外部控制平面在较长时间内重新配置核心分配。

10 Conclusion

Shenango是一个系统，可以在处理多个延迟敏感和批处理应用程序的机器上同时保持CPU效率、低尾延迟和高网络吞吐量。Shenango通过其IOKernel实现这些优势，IOKernel是一个专用核心，与网络集成，驱动应用程序之间的细粒度核心分配调整。IOKernel利用拥塞检测算法，可以通过跟踪数据包和应用程序线程的排队积压信息，在微秒级时间尺度上对应用程序超载做出反应。这种设计使Shenango能够显著改进以前的内核绕过网络堆栈，通过消除最小和峰值负载之间的供应差距，回收因繁忙自旋而浪费的周期。最后，我们的每个应用程序运行时通过提供高级编程抽象（如轻量级线程和同步网络套接字）以低开销的方式，使这些优势更易于开发人员使用。

11 Acknowledgments

感谢KyoungSoo Park牧羊人、匿名审稿人、John Ousterhout、Tom Anderson、Frans Kaashoek、Nickolai Zeldovich和PDOS的其他成员的有用反馈。感谢Henry Qin帮助我们评估Arachne。Amy Ousterhout得到了NSF Fellowship和Hertz Foundation Fellowship的支持。这项工作部分资助来自Google Faculty Award和NSF Grants CNS-1407470、CNS-1526791和CNS-1563826。
