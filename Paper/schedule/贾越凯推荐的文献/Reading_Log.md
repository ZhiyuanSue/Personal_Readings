以下是阅读记录



# 1、Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads

Shenango

简单来说就是在一个核心上面专门用于做某个IOkernel，还有核心分配任务，别的用户进程啥的就是不做



只有当核心处于**空闲状态**（第2行），或者**没有空闲核心且正在使用核心的应用程序正在爆发**（使用超过其保证的核心数量）时，核心才有资格被分配

在符合条件的核心中，选择算法SELECT CORE首先尝试分配**应用程序当前正在使用的核心的超线程对**，它尝试分配该应用程序**最近使用但不再使用的核心**，最后，算法**选择任何空闲核心（如果存在）**，或者从爆发应用程序中选择一个随机核心。



Shenango使用Packet steering技术，通过IOKernel跟踪每个运行时的核心，将传入的数据包直接传递到运行相应运行时的核心。**每个运行时都有自己的IP和MAC地址，IOKernel通过哈希表查找MAC地址来确定运行时**，并使用**RSS哈希选择核心**，将数据包排队到该核心的入站数据包队列中。Shenango可以通过技术如Intel的Flow Director或FlexNIC进一步优化数据包转发。



但总而言之，我个人的看法是，使用高频率的核心调度，以及通过内核旁路的技术，让他相比于原生linux的高收包性能变得可行。





# 2、Efficient Scheduling Policies for Microsecond-Scale Tasks

主要是一个是延迟，一个是效率，两者需要进行一个平衡，核心分配的越多，延迟的就越低，因为会更快有核心响应，但是同样的，性能浪费的越多

本文主要考虑两个，一个是负载均衡，一个是核心分配

负载均衡策略可以在任务到达时或已经排队时进行负载均衡

1、单队列负载均衡策略理论上是最理想的，但实际上由于对单一队列的争用而限制了吞吐量

2、没有负载均衡，任务由它们最初到达的核心处理，没有负载均衡开销

3、在任务创建时决定将任务分配给哪个核心；任务创建后不能再移动。现有系统通常使用“二选一”策略，将任务排入两个随机选择的核心中负载较轻的那个核心。任务创建时，创建核心需要花费额外开销来对其他核心的队列进行采样（对于少量采样的核心可以并行进行），并将任务排入所选择的核心

4、工作窃取，需要有检查其他核心的开销

5、工作分配。过载的核心可以将负载分配给其他核心或请求其他核心承担部分负载，和工作窃取差别在于一个是谁主动。



核心分配策略，就是要给每个应用程序分配多少个核心

1、静态分配策略，不会产生核心重新分配的开销，每个应用程序必须分配足够的核心以应对峰值负载，这浪费了大量的CPU资源

2、Fred系统，每个任务分配一个核心的方式，每次任务到来都会分配一个核心，但这会增加每个任务的核心分配开销

3、基于排队的策略，根据排队延迟来授予应用程序额外的核心，

4、基于CPU利用率的策略

在某些系统中，当核心无法找到任何任务可执行时，应用程序会产生一个核心



除此之外，本文还设计了两个新的核心分配策略：

1、延迟范围（delay range）就是尽可能让一个应用进程在所有核心上保持同样的平均排队时延，每5μs检查一次，低于下限减少内核，高于上限，增加内核

2、利用率范围（utilization range），使用CPU利用率作为指标。



实测的开销

负载均衡需要核之间的通信，主要由于**从另一个核的L2缓存中检索缓存行而导致缓存未命中的开销**。根据CPU微架构的不同，这样的缓存未命中的成本可能在30 ns（Intel Haswell）到200 ns（Xeon Phi）之间。

核心分配开销。现有系统报告的核心分配延迟略高，从Shenango中重新分配空闲核心需要**2.2微秒**，重新分配繁忙核心需要**7.4微秒**



未考虑的问题

1、专用一个核心当调度器（和第一篇论文不同）

2、测试本身会导致一定的延迟和开销

3、没考虑安全性



结论是，负载均衡最好的是工作窃取

而核心分配，对于任何短任务，啥动态分配都比不上静态分配。对于大于10微秒或100微秒的任务时，这种动态分配才能体现出优势

而本文设计的延迟范围和利用率范围也都表现很好



相关工作，建议了解

# 3、Fast & Flexible User-Space Delegation of Linux Scheduling

ghOSt是一个在用户空间运行的内核调度程序，用于调度本地操作系统线程

但是按照这个描述，我认为这说的是，设计了一个机制，让主机内核可以动态的更新调度策略



大概得意思是，内核默认的实现了CFS这些东西，然后使用一个用户空间的agent，来进行内核调度程序的管理，如果agent崩了，就继续回到CFS

或者重新启动agent

然后可以动态的改agent的策略



关于调度的粒度

每个CPU都有一个agent，但是可以有按照CPU为单位的调度或者一个集群的调度

后者，只有一个全局调度可以用，其他的都是不可用的。



内核必须向agent程序公开线程状态，可以将数据结构映射到用户空间，或者通过sysfs，而作者选择了消息方式

具体消息细节我觉得无关紧要



agent和内核同步，意思是，内核更新状态的时候，agent可能收到新的消息，所以用了一个序列号的机制，来防止这种事情发生（问，如果发生了溢出呢？比如运行一段时间比如几年之后或者消息过多之后？？？这在服务器中是很容易做到的）

而agent通过类似于提交事务一样的机制将决策提供给CPU

为了减少开销，引入组提交



其他相关实现细节非常多，如下列出了一些，但是我觉得都是实现细节。

ghOSt采用单一全局代理和单一队列进行集中调度，该代理轮询单一消息队列并为所有受其管理的CPU做出调度决策。尽管集中策略可能看起来无法支持微秒级别的调度，但在生产工作负载上，ghOSt表现出相当或更好的整体性能。

为了支持微秒级调度，全局代理必须持续运行，避免被更高优先级的内核调度类抢占。为了防止这种情况，ghOSt将所有代理分配高内核优先级，但这可能会不稳定系统。因此，必须小心处理，以确保系统稳定。

ghOSt通过让所有不活跃的代理立即让出CPU来维护系统的稳定性。当非ghOSt线程需要在全局代理的CPU上运行时，全局代理会将其“热交接”给另一个空闲的CPU上的不活跃代理。这样可以保证系统的稳定性和线程的顺畅运行。

全局代理可能会出现线程状态不一致的情况，需要一种机制来确保在调度线程时，如果有线程亲和性的限制，事务将失败。

全局代理需要支持数千个线程，不像本地代理那样放弃自己的CPU。全局代理只需要验证它是否与当前调度的线程T保持最新即可。可以使用代理序列号来实现。

使用线程序列号解决问题。每个排队的消息都会标记上线程序列号。当代理为线程T提交事务时，它会将事务和它所知道的线程T的最新序列号一起发送。内核接收到事务后，会验证线程序列号是否与事务中的线程一致。如果不一致，则事务失败并返回ESTALE错误。



# 4、Mitigating Interference at Microsecond Timescales

他自己也说了，跟第一篇论文的Shenango差不太多

但也有一些差别

Shenango只使用排队延迟作为控制信号，Caladan使用多个信号

Caladan的调度核心只负责CPU调度，而Shenango的调度核心将网络处理与CPU调度结合在一起



用户可以为每个任务分配一定数量的保证核心数，这些核心数在需要时始终可用。用户还可以为任务分配额外的可突发核心数，以利用任何空闲容量。此外，每个任务被指定为LC（latency-critical）或BE（low-priority,best-effort）。BE任务的优先级较低：只有在LC任务不需要时，它们才被分配可突发核心数，它们始终分配零个保证核心数，并根据需要进行限制以管理干扰。



对于用于控制调度的信号

三个来源：

运行时间提供有关请求处理时间和排队延迟的信息；

DRAM控制器提供有关全局内存带宽使用情况的信息；

KSCHED提供有关每个核心LLC缺失率的信息，并在任务自愿放弃时通知调度器核心



对于排队时延

它会定期检查每个任务的排队延迟，并尝试向延迟超过可配置的每个任务阈值（THRESH_QD）的任务添加核心。

排队可以发生在每个运行时核心的绿色线程运行队列、网络入口队列、存储完成队列和计时器堆中。每个排队元素都包含其到达时间的时间戳，并且所有队列都放置在共享内存中。QueueingDelay()通过将每个队列中最老元素的延迟相加来计算每个核心的延迟。然后它报告任务核心中观察到的最大延迟。



对于DRAM控制器

定期轮询DRAM控制器的全局内存带宽使用计数器，计算自上次轮询间隔以来的访问速率，并在其超过饱和阈值时触发。

从每个调度核的性能监控单元（PMU）高效采样LLC misses来将内存带宽使用归因于特定任务



超线程控制器

检测到超线程干扰后，禁止使用兄弟超线程直到当前请求完成



# 5、Preemptive Scheduling for μsecond-scale Tail Latency

先批判了一番几个调度

分布式排队和先进先出（FCFS）调度，称为d-FCFS

ZygOS用c-FCFS



# 6、User-De￿fined Scheduling Across the Stack



# 7、When Idling is Ideal Optimizing Tail-Latency for Heavy-Tailed Datacenter Workloads with Perséphone
