本文是OSDI22的IPADS实验室的一篇论文 

# 背景

在例如智能驾驶这样的场景上，一块GPU可能被用于多个任务

这个任务他可能包括对相机识别的周围场景进行AI的运算，对驾驶员疲劳的识别，语音的识别等等

所以就是多个任务需要共享一个GPU

那么这些任务有的是实时的（可以理解为硬实时）

有些任务是尽最大努力交付的



因此这样有两个目标

一个是实时任务应当在GPU上面不受其他任务的干扰尽可能快的完成

其次是

实时任务和尽最大努力交付的任务都应当并发



这个图

a是说他实现的这个REEF做的这个吞吐量，基本都是随着任务规模扩大线性增长

b是说他现有的问题，就是，随着尽最大努力交付的任务数量上升，实时任务的延迟会增长非常高，50ms的延迟是不可接受的

c是说抢占延迟会随着抢占内核的数量而增加

d是另一个问题，就是随着实时调度任务的高频率发生，他会导致一些尽最大努力交付的任务，饿死了



但是，和CPU本身就设计了调度这一东西不同，GPU并没有针对这种任务的调度（不能说GPU没有调度）

英伟达说自从Pascal架构开始就具备了抢占的支持，但是作者说找不到公开的信息和软件接口



于是设计了REEF，采用微秒级别的调度

思路是说，实时任务如果到达，可以直接从尽最大可能交付的任务手中抢掉GPU，然后剩下的时间交给最大可能交付的任务。



一个核心观点是说，DNN是幂等的（虽然我不知道啥意思），反正意思就是我可以不保存上下文，直接替换掉他现在在跑的任务，到时候直接还原回去，所以REEF设计了一个基于重置的抢占方案。他改造了驱动以及一堆软件队列，让他可以在几十微秒的时间内启动一个实时任务。

那之前的工作就是说，我尽可能的等待之前的任务完成。



除此之外，还做了一个动态内核填充的方法，动态的去把某个最优的GPU的kernel绑定到实时任务的kernel



## DNN的特性

幂等性

除了幂等性之外，一个特点是**Massive kernels**的问题，现在的模型规模很大，一般来说会一口气把一堆kernel交给GPU，如果发生GPU任务抢占，这些kernel会导致巨大的开销。



可以预测的延迟

因为都是没有逻辑分支和状态的代数运算，所以，可以根据矩阵规模和使用的算法，推测出固定规模的运算需要多少时间

不同的并行度

由于不同的输入尺寸，有不同的并行度

图三表示了可以预测的延迟，基本上当矩阵给定的时候，执行时间都是差不多的（一整条竖线



图四则表示了不同并行度的问题

给pooling计算核分配了64个GPU的blocks，然后给softmax计算核只分配了一个CU

结果实际过程中，他的CU利用率不断地变动。





## 现有工作

目前在HPC领域对于GPU的调度包括如下几种

先看这个示意图吧，感觉就是每个任务随着时间，不同的任务需要多少的CU

顺序执行

使用顺序执行来避免任务干扰，所以执行的延迟会很低（因为使用了全部的资源）但是有个问题，有抢占的延迟，必须等待先前的任务完成

另外，不能充分的去利用GPU，并发低，吞吐量差



block层面的调度

需要等待当前的block执行完，然后再执行当前到达的实时任务

缺点是，一个是如图1C，抢占延迟会随着被抢占内核数量而增加

而且当初的这个设计必须限制提交给GPU的内核的数量

高频率的实时任务会破坏尽最大可能交付的任务，甚至如图14那样导致饥饿



多个GPU流

现代的GPU库比如CUDA或者ROCm运行调度器会按照需要从GPU流中调度内核保证所有CU是忙碌的，但是缺点是，这种调度有可能导致实时任务的延迟过高，比如图中的最后一个RT任务的内核



REEF的架构

一个是快速可重复的抢占

另一个是动态内核填充

他包括两个部分

一个是离线部分用于编译和加载用户的DNN模型

另一个是在线部分，用于调度和服务DNN的推理请求



离线部分

这部分除了编译加载之外，还做了一个计算该模型大概的执行时间的操作



在线部分

包括4个部分

task queue

scheduler

preemption module

Dynamic kernel padding






