没有下原文就给个链接吧

https://www.usenix.org/system/files/osdi18-cutler.pdf

然后这门课，我还在知乎发现个翻译的老哥，说实在的直接看代码或者英文教程也确实会头痛的，我贴一下

https://www.zhihu.com/column/c_1294282919087964160



# 摘要

使用带有GC的HLL来实现一个宏内核的POSIX接口的内核。然后探索性能表现，实现难度以及可编程性和安全性等等。

使用Go语言的源码分析来避免跑飞



对于花费在Go的HLL特征比如GC上的消耗，大概在13%左右。

用在NGINX上面花费最长的GC相关的停顿大概在115ms

一个NGINX client的请求则花费600ms。

总体而言，相比于C大概多了5%到15%



# 简介

C有一大堆的问题，HLL则有内存安全，类型安全以及线程抽象等等优点。

Go的静态分析可以分析每个系统调用需要多少内存，一旦允许系统调用，那么这个系统调用的分配一定可以成功。（是个好想法，就是不知道中断嵌套怎么整？？？）

反正整篇文章做了这么几个事情

- 实现了小饼干

- 应对内核堆的新方案

- 讨论内核中使用HLL的定性分析是否有帮助

- 衡量性能

- 比较GO和C
  
  

# 相关工作

在此之前有Pilot和Lisp的内核

Mesa缺少了垃圾回收，但是他的继承语言Cedar则有，LISP则存在一个实时的GC

一大堆的研究性的内核是基于高级语言的

性能当然是个问题，但是这些研究性的内核通常关系到新的idea而不是语言问题。



Gopher OS也是一个Go写的操作系统，但是还在早期开发阶段。

其他的也有，但是并不是为了和biscuit一样的目的。

例如Clive是个unikernel，Ethos OS使用C写Kernel然后go用来用户空间编程，目的是安全。gVisor是个用户空间内核。



对于内存回收，没有一个共识说一定要有或者一定没有，比如Rust就觉得效率不高，然后编译器自动释放内存。但是这又导致了多个线程和闭包之间共享数据变得尴尬。

也有人研究过在Linux内核中添加GC，但是他发现内核环境让这个任务非常困难。



内核堆用完的情况

Linux内核堆用完之后，会等待并且重新试几次但是不可能无限等待，否则会死锁。因此Linux内核必须包含从分配错误中recover的代码。



biscuit的内核分配不会挂掉，因为他一开始就分配好了足够多的内存。只不过会等待。



# motivation

为什么选择C

性能挺重要



HLL

好理解

减少buffer overrun，use after free，类型不安全问题。

但是仍然要有成本



# overview

大概介绍了biscuit的框架。（反正都是OS常见的东西，看过一遍算了）

最底下是个垫片shim层，感觉就像是HAL？？？

runtime

中断

多核和同步异步问题

虚拟内存

文件系统

网络栈

限制：没有优先级因为他依赖于go的运行时调度器。以及为一些数量小的内核做了优化，但是没有考虑NUMA情况下的多核行为。没有swap，也没有反向映射（reverse-mapping）

反向映射本身也是个蛮复杂的技术（。。。头一次听说）



# GC

对于GC的其他研究，详见另一个文件夹对于GC的阅读

在看的时候，看到一个问题，那就是在本文的代码中，貌似用的是go1.4版本的，但是在1.5版本才加入了三色标记法，在1.8版本才用了混合屏障技术，在论文中明确说了，使用的是go1.1版本的并行标记清除法。这里面是否有性能的差距？？？但是后面的介绍里面，又具体提到了写屏障的技术。



# 避免heap exhaustion

biscuit试图把那些超额分配内核堆空间的对象给清理掉。

他有三种办法

一个是快要用完的时候清理cache

第二个是system call的时候。如果快用完了，就推迟，直到能够找到足够内存。

第三个是弄一个kill线程，检测超额调用内核堆空间的线程并杀掉他们。



biscuit保留空间的算法

他用了g，c和n

g这个东西比较复杂，还得跟GC一起考虑，因为GC分为几个阶段，有可能会处在标记之后并没有清理的阶段，这时候这些空间还没有回收，所以仍然被占用

c是已经完成的系统调用还保留的内容

n是还在干活的系统调用的保留空间大小



L是这三个加起来（

L+s需要小于M

然后n加上s，否则就调用kill线程



结束的时候计算实际调用的空间大小a

a+c都清理掉

否则如果用了超过s（为啥可以这样？？）就清理掉s+c



分开c和n是因为垃圾收集



如果heap内存充足，也就是活动数据远小于M，剩下的全是死内存，g+n接近M

这时候需要在killer线程在决定终止某个线程之前执行一次收集



对于s的静态分析

s就是system call需要的空间



反正没看懂，不懂啥叫。。。当然也有看懂了的部分，比如避免递归等等



使用调用图，SSA分析，去弄清楚每个函数调用分配内存的情况。



go的逃逸分析

逃逸就是本该分配到栈上的分配到堆上了。

然后go有个这个分析之后可以让他弄回栈上，提升gc的性能。



另一个问题是如果碰到loop怎么办，就是那种无法确定迭代次数，或者最大迭代次数可能很大的情况，怎么确定s

就是每次迭代的时候都尽量确保有足够空间，如果没有足够的空间，就abort并等待。

并且如果失败了，有两个loop需要回退



exit，fork和exec都是非常挑战性的loop

原因是，会关掉一大堆的文件描述符。这会导致栈占用的内存数量爆炸。

于是他进行了一个close的函数（？），然后在函数调用返回的时候检查是否可以有足够空间用于分配



然后close系统调用是作者唯一一个手工分析空间分配的系统调用



内核线程

一个例子是文件系统日志线程，他会一直跑

每个这样的长期运行的内核线程都会有自己的堆保留。



Killer 线程

他先启用一个GC

如果GC不能释放足够多的空间，killer释放足够的cache空间，然后再进行收集

然后找到占用空间最大的那个，杀掉他

然后再GC

然后再唤醒需要空间的那个线程。



限制

大概意思是，GC本身也需要堆栈空间，但是小饼干可以一些办法从这种方式中恢复

所以他们也计算了一下，GC最多需要堆RAM0.8%的空间



碎片问题

尽可能避免



# 具体实现

大概说了一堆用了多少多少go代码巴拉巴拉



修改了Go runtime，为了记录堆空间大小然后计算好分配的bytes



保留了go运行时和编译器的一些属性，当如果持有锁或者go runtime的私有状态不会关闭中断。

为了防止死锁，只设置了一个标志



而且运行时本身可能进行上下文切换，所以定时器中断无法强制上下文切换



因为额，go runtime本身有抢占机制



biscuit本身不控制scheduling策略，因为Goruntime本身有个调度。



而且他还改了GC线程，从而没有线程跟用户线程竞争



biscuit还使用了大页表来减少内核iTLB的miss



# Evaluation

啊，反正就是进行一些性能比较。我不是很care这个



但是他说C比go要快15%

问题是就是前面我提到的，go1.5和1.8版本的gc有了很大的改进，我也不知道后续有没有更新性能。
